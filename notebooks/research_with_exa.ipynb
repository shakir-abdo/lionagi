{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to have an EXA_API_KEY, get one at [exa.ai](https://exa.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "import json\n",
    "\n",
    "from lionagi import iModel, Branch, types, BaseModel\n",
    "from lionagi.service.providers.types import ExaSearchRequest\n",
    "from lionagi.utils import alcall\n",
    "\n",
    "\n",
    "class SearchRequests(BaseModel):\n",
    "    search_requests: list[ExaSearchRequest] = []\n",
    "\n",
    "\n",
    "class Analysis(BaseModel):\n",
    "    analysis: str\n",
    "\n",
    "\n",
    "class Source(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "\n",
    "\n",
    "class ResearchDraft(BaseModel):\n",
    "    title: str\n",
    "    content: str\n",
    "    source: list[Source]\n",
    "\n",
    "\n",
    "# need to have an EXA_API_KEY, get one at exa.ai\n",
    "exa = iModel(\n",
    "    provider=\"exa\",\n",
    "    endpoint=\"search\",\n",
    "    queue_capacity=5,\n",
    "    capacity_refresh_time=1,\n",
    "    invoke_with_endpoint=False,\n",
    ")\n",
    "\n",
    "\n",
    "async def research(\n",
    "    branch: Branch,\n",
    "    query: str,\n",
    "    domain: str | None = None,\n",
    "    style: str | None = None,\n",
    "    sample_writing: str | None = None,\n",
    "    interpret_kwargs: dict | None = None,\n",
    "    *,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    High-level research operation with optional verbose printing:\n",
    "      1) Interpret user query.\n",
    "      2) Generate an analysis from the LLM.\n",
    "      3) Produce search requests and call the EXA provider (cached).\n",
    "      4) Transform search results (compressed text).\n",
    "      5) Prepare a final draft/summary.\n",
    "\n",
    "    Returns an OperableModel with fields:\n",
    "      analysis, search_requests, search_results, transformed_results, draft.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    branch : Branch\n",
    "        The branch instance handling the conversation / context.\n",
    "    query : str\n",
    "        The user's research query.\n",
    "    domain : Optional[str]\n",
    "        Domain hint (e.g. \"finance\", \"marketing\").\n",
    "    style : Optional[str]\n",
    "        Style hint (e.g. \"concise\", \"technical\").\n",
    "    sample_writing : Optional[str]\n",
    "        A sample snippet that might help interpret the style or structure.\n",
    "    interpret_kwargs : Optional[dict]\n",
    "        Additional parameters for the `branch.interpret()` call.\n",
    "    verbose : bool\n",
    "        If True, prints intermediate results step-by-step.\n",
    "    kwargs : dict\n",
    "        Additional arguments passed along if needed.\n",
    "    \"\"\"\n",
    "\n",
    "    out = types.OperableModel()\n",
    "    try:\n",
    "        if interpret_kwargs is None:\n",
    "            interpret_kwargs = {}\n",
    "\n",
    "        # -- Step 1: Interpret the query for better clarity --\n",
    "        interpreted = await branch.interpret(\n",
    "            text=query,\n",
    "            guidance=\"Rewrite the user input to ensure we fully understand and clarify the user's objective.\",\n",
    "            domain=domain,\n",
    "            style=style,\n",
    "            sample=sample_writing,\n",
    "            **interpret_kwargs,\n",
    "        )\n",
    "        if verbose:\n",
    "            display(Markdown(f\"**[Step 1] Interpreted query:** {interpreted}\"))\n",
    "\n",
    "        # -- Step 2: Generate analysis from the LLM --\n",
    "        analysis = await branch.operate(\n",
    "            instruction=interpreted,\n",
    "            guidance=(\n",
    "                \"Perform a thorough analysis focusing on domain knowledge, \"\n",
    "                \"potential angles, and constraints. Be concise but complete.\"\n",
    "            ),\n",
    "            response_format=Analysis,\n",
    "            reason=True,\n",
    "        )\n",
    "        out.add_field(\"analysis\", analysis, annotation=Analysis)\n",
    "        if verbose:\n",
    "            display(\n",
    "                Markdown(f\"**[Step 2] Analysis result:** {analysis.analysis}\")\n",
    "            )\n",
    "\n",
    "        # -- Step 3: Produce search requests based on the analysis --\n",
    "        search_requests: SearchRequests = await branch.operate(\n",
    "            instruction=(\n",
    "                \"Based on the analysis, produce a list of relevant search requests \"\n",
    "                \"for the EXA provider. Focus on the key points from the analysis.\"\n",
    "                \"make sure you get sufficient information from the search results.\"\n",
    "                \"should get summaries of the articles as well as the sufficient text \"\n",
    "                \"from the articles. exclude sites like reddit, or other low quality sources.\"\n",
    "            ),\n",
    "            guidance=(\n",
    "                \"Generate specific queries that capture the key aspects from the analysis. \"\n",
    "                \"Provide enough detail for each request.\"\n",
    "            ),\n",
    "            response_format=SearchRequests,\n",
    "            reason=True,\n",
    "        )\n",
    "        out.add_field(\n",
    "            \"search_requests\", search_requests, annotation=SearchRequests\n",
    "        )\n",
    "        if verbose:\n",
    "            display(Markdown(f\"**[Step 3] Search requests:**\"))\n",
    "            for i in search_requests.search_requests:\n",
    "                display(\n",
    "                    Markdown(\n",
    "                        f\"{i.model_dump_json(exclude_none=True, indent=2)}\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Prepare API calls\n",
    "        api_calls = []\n",
    "        for req in search_requests.search_requests:\n",
    "            params = req.model_dump(exclude_none=True)\n",
    "            # Ensure we cache search results by default\n",
    "            params[\"is_cached\"] = params.get(\"is_cached\", True)\n",
    "            api_call = exa.create_api_calling(**params)\n",
    "            api_calls.append(api_call)\n",
    "\n",
    "        # Invoke EXA searches asynchronously\n",
    "        search_results = await alcall(\n",
    "            api_calls, exa.invoke, retry_default=None, dropna=True\n",
    "        )\n",
    "        out.add_field(\n",
    "            \"search_results\",\n",
    "            [res.response for res in search_results],\n",
    "            annotation=list[dict],\n",
    "        )\n",
    "        if verbose:\n",
    "            display(Markdown(f\"**[Step 3] Search results:**\"))\n",
    "            for res in search_results:\n",
    "                display(\n",
    "                    Markdown(\n",
    "                        \"\\n\".join(\n",
    "                            \"  - \" + i[\"title\"]\n",
    "                            for i in res.response[\"results\"]\n",
    "                            if \"title\" in i and i[\"title\"]\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # -- Step 4: Draft a final output referencing the transformed results --\n",
    "        draft = await branch.operate(\n",
    "            instruction=(\n",
    "                f\"Prepare a well-formatted, factual {style} research report basing on the research findings. \"\n",
    "                \"Incorporate key insights from the context.\"\n",
    "            ),\n",
    "            guidance=(\n",
    "                \"Synthesize a final report using all insights gleaned from the search results. \"\n",
    "                \"Ensure clarity and accuracy, and follow any requested style.\"\n",
    "                \"answer the user's question, and provide additional context.\"\n",
    "            ),\n",
    "            context={\n",
    "                \"search_results\": json.dumps(\n",
    "                    [res.response for res in search_results]\n",
    "                )\n",
    "            },\n",
    "            response_format=ResearchDraft,\n",
    "        )\n",
    "        out.add_field(\"research_draft\", draft, annotation=ResearchDraft)\n",
    "        if verbose:\n",
    "            display(Markdown(f\"**[Step 4] Draft:** \\n\\n{draft.content}\"))\n",
    "            for i in draft.source:\n",
    "                display(Markdown(f\"**Source:** [{i.title}]({i.url})\"))\n",
    "    except Exception as e:\n",
    "\n",
    "        out.add_field(\"error\", str(e), annotation=str)\n",
    "        print(\"Error occurred during research:\", e)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher_prompt = \"\"\"SYSTEM PROMPT (Researcher):\n",
    "You are a specialized research assistant, trained to gather information from various sources accurately and concisely. Your job involves:\n",
    " • Interpreting user questions and clarifying objectives,\n",
    " • Proposing relevant angles or methods of inquiry,\n",
    " • Generating precise search queries to explore any topic,\n",
    " • Summarizing findings accurately while preserving key details.\n",
    "\n",
    "When performing your tasks:\n",
    " • Confirm context and constraints (like domain or style requirements).\n",
    " • Provide well-structured, consistent, and thorough analyses.\n",
    " • Use suitable search queries to gather relevant info.\n",
    " • Summarize or compress results in a way that remains factual.\n",
    " • Maintain an objective, knowledgeable, and professional tone.\n",
    "\n",
    "Overall, your responsibility is to produce high-quality research findings and drafts that help the user solve problems or gather insights effectively.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of `research` function\n",
    "\n",
    "from lionagi.session.branch import Branch\n",
    "\n",
    "haiku = iModel(\n",
    "    provider=\"openrouter\",\n",
    "    model=\"anthropic/claude-3.5-haiku\",\n",
    "    max_tokens=8000,  # required for anthropic models\n",
    "    invoke_with_endpoint=False,\n",
    ")\n",
    "\n",
    "sonnet = iModel(\n",
    "    provider=\"openrouter\",\n",
    "    model=\"anthropic/claude-3.5-sonnet\",\n",
    "    max_tokens=8000,  # required for anthropic models\n",
    "    invoke_with_endpoint=False,\n",
    ")\n",
    "\n",
    "researcher = Branch(\n",
    "    system=researcher_prompt,\n",
    "    chat_model=sonnet,\n",
    "    parse_model=haiku,\n",
    ")\n",
    "\n",
    "# Example query requesting an analysis of LLM-based summarization in finance\n",
    "query_text = (\n",
    "    \"I want to compare different LLM-based summarization approaches \"\n",
    "    \"for financial documents. Focus on accuracy, cost, and domain adaptability. \"\n",
    "    \"Also highlight practical use-cases or references.\"\n",
    ")\n",
    "\n",
    "# Optional style or domain hints\n",
    "domain_hint = \"finance\"\n",
    "style_hint = \"extensive\"\n",
    "sample_snippet = (\n",
    "    \"Sample text: In the finance domain, we often handle massive amounts of data. \"\n",
    "    \"We want a method that can summarize quickly and accurately.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**[Step 1] Interpreted query:** Here's the clarified and structured prompt:\n",
       "\n",
       "Please provide a comprehensive analysis of different Large Language Model (LLM) approaches for summarizing financial documents, addressing the following key aspects:\n",
       "\n",
       "1. Comparison of summarization methods:\n",
       "   - Traditional LLM summarization techniques\n",
       "   - Finance-specific LLM models and fine-tuning approaches\n",
       "   - Hybrid approaches combining LLMs with rule-based systems\n",
       "\n",
       "2. Evaluation criteria:\n",
       "   - Accuracy metrics (precision, recall, F1 score) for financial content\n",
       "   - Cost analysis (computational resources, API costs, training expenses)\n",
       "   - Domain adaptation capabilities for different financial document types (earnings reports, SEC filings, research papers)\n",
       "\n",
       "3. Practical considerations:\n",
       "   - Real-world implementation examples in financial institutions\n",
       "   - Handling of specialized financial terminology and numerical data\n",
       "   - Compliance and regulatory requirements\n",
       "\n",
       "4. Use cases and applications:\n",
       "   - Investment research automation\n",
       "   - Regulatory filing analysis\n",
       "   - Financial news summarization\n",
       "   - Risk assessment documentation\n",
       "\n",
       "Please include relevant academic papers, industry case studies, or benchmarks that support the analysis. Also address any limitations or challenges specific to financial domain summarization."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[Step 2] Analysis result:** Domain Knowledge Analysis:\n",
       "1. Financial Document Characteristics:\n",
       "- Heavy use of numerical data and financial metrics\n",
       "- Specialized terminology and regulatory language\n",
       "- Strict accuracy requirements due to compliance\n",
       "- Multiple document types with varying structures\n",
       "\n",
       "2. Technical Approaches:\n",
       "a) Traditional LLM Methods:\n",
       "- Base models like GPT, BERT\n",
       "- Pros: General text understanding\n",
       "- Cons: Miss financial nuances\n",
       "\n",
       "b) Finance-Specific Models:\n",
       "- FinBERT, FinGPT\n",
       "- Domain-adapted architectures\n",
       "- Better financial terminology handling\n",
       "\n",
       "c) Hybrid Systems:\n",
       "- Rule-based + LLM combination\n",
       "- Enhanced numerical accuracy\n",
       "- Structured data extraction\n",
       "\n",
       "3. Key Constraints:\n",
       "- Regulatory compliance (SEC, FINRA)\n",
       "- Data privacy requirements\n",
       "- Real-time processing needs\n",
       "- Cost-effectiveness at scale\n",
       "\n",
       "4. Success Metrics:\n",
       "- Factual accuracy preservation\n",
       "- Numerical precision\n",
       "- Context retention\n",
       "- Processing speed\n",
       "\n",
       "5. Implementation Considerations:\n",
       "- API integration requirements\n",
       "- Model hosting infrastructure\n",
       "- Data security protocols\n",
       "- Monitoring systems"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[Step 3] Search requests:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "{\n",
       "  \"query\": \"comparison of finance-specific LLMs vs traditional LLMs for financial document summarization benchmarks AND performance metrics\",\n",
       "  \"category\": \"research paper\",\n",
       "  \"type\": \"neural\",\n",
       "  \"useAutoprompt\": false,\n",
       "  \"numResults\": 15,\n",
       "  \"excludeDomains\": [\n",
       "    \"reddit.com\",\n",
       "    \"medium.com\"\n",
       "  ],\n",
       "  \"startPublishedDate\": \"2022-01-01T00:00:00.000Z\",\n",
       "  \"contents\": {\n",
       "    \"text\": {\n",
       "      \"includeHtmlTags\": false,\n",
       "      \"maxCharacters\": 10000\n",
       "    },\n",
       "    \"highlights\": {\n",
       "      \"highlightsPerUrl\": 2,\n",
       "      \"numSentences\": 3\n",
       "    },\n",
       "    \"summary\": {},\n",
       "    \"livecrawl\": \"never\",\n",
       "    \"livecrawlTimeout\": 10000\n",
       "  }\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "{\n",
       "  \"query\": \"financial document summarization hybrid approaches combining LLMs with rule-based systems implementations case studies\",\n",
       "  \"category\": \"research paper\",\n",
       "  \"type\": \"neural\",\n",
       "  \"useAutoprompt\": false,\n",
       "  \"numResults\": 10,\n",
       "  \"excludeDomains\": [\n",
       "    \"reddit.com\",\n",
       "    \"medium.com\"\n",
       "  ],\n",
       "  \"startPublishedDate\": \"2021-01-01T00:00:00.000Z\",\n",
       "  \"contents\": {\n",
       "    \"text\": {\n",
       "      \"includeHtmlTags\": false,\n",
       "      \"maxCharacters\": 8000\n",
       "    },\n",
       "    \"highlights\": {\n",
       "      \"highlightsPerUrl\": 1,\n",
       "      \"numSentences\": 3\n",
       "    },\n",
       "    \"summary\": {},\n",
       "    \"livecrawl\": \"never\",\n",
       "    \"livecrawlTimeout\": 10000\n",
       "  }\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "{\n",
       "  \"query\": \"regulatory compliance requirements for LLM financial document summarization SEC FINRA guidelines\",\n",
       "  \"category\": \"news\",\n",
       "  \"type\": \"neural\",\n",
       "  \"useAutoprompt\": false,\n",
       "  \"numResults\": 10,\n",
       "  \"excludeDomains\": [\n",
       "    \"reddit.com\",\n",
       "    \"medium.com\",\n",
       "    \"quora.com\"\n",
       "  ],\n",
       "  \"startPublishedDate\": \"2022-01-01T00:00:00.000Z\",\n",
       "  \"contents\": {\n",
       "    \"text\": {\n",
       "      \"includeHtmlTags\": false,\n",
       "      \"maxCharacters\": 5000\n",
       "    },\n",
       "    \"highlights\": {\n",
       "      \"highlightsPerUrl\": 1,\n",
       "      \"numSentences\": 2\n",
       "    },\n",
       "    \"summary\": {},\n",
       "    \"livecrawl\": \"never\",\n",
       "    \"livecrawlTimeout\": 10000\n",
       "  }\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "{\n",
       "  \"query\": \"FinBERT FinGPT financial domain adaptation techniques evaluation metrics accuracy\",\n",
       "  \"category\": \"research paper\",\n",
       "  \"type\": \"neural\",\n",
       "  \"useAutoprompt\": false,\n",
       "  \"numResults\": 12,\n",
       "  \"excludeDomains\": [\n",
       "    \"reddit.com\",\n",
       "    \"medium.com\"\n",
       "  ],\n",
       "  \"startPublishedDate\": \"2021-01-01T00:00:00.000Z\",\n",
       "  \"contents\": {\n",
       "    \"text\": {\n",
       "      \"includeHtmlTags\": false,\n",
       "      \"maxCharacters\": 8000\n",
       "    },\n",
       "    \"highlights\": {\n",
       "      \"highlightsPerUrl\": 1,\n",
       "      \"numSentences\": 3\n",
       "    },\n",
       "    \"summary\": {},\n",
       "    \"livecrawl\": \"never\",\n",
       "    \"livecrawlTimeout\": 10000\n",
       "  }\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[Step 3] Search results:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "  - FinanceBench: A New Benchmark for Financial Question Answering\n",
       "  - L3iTC at the FinLLM Challenge Task: Quantization for Financial Text Classification & Summarization\n",
       "  - DocFinQA: A Long-Context Financial Reasoning Dataset\n",
       "  - Numerical Reasoning for Financial Reports\n",
       "  - Towards Optimizing the Costs of LLM Usage\n",
       "  - Zero-Shot Question Answering over Financial Documents using Large Language Models\n",
       "  - Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective\n",
       "  - LongFin: A Multimodal Document Understanding Model for Long Financial Domain Documents\n",
       "  - BizBench: A Quantitative Reasoning Benchmark for Business and Finance\n",
       "  - Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models\n",
       "  - Multimodal Gen-AI for Fundamental Investment Research\n",
       "  - Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing\n",
       "  - Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications\n",
       "  - Beyond Classification: Financial Reasoning in State-of-the-Art Language Models\n",
       "  - A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "  - Information Extraction through AI techniques: The KIDs use case at CONSOB\n",
       "  - Numerical Reasoning for Financial Reports\n",
       "  - An AI-based Approach for Tracing Content Requirements in Financial Documents\n",
       "  - NLP-based Decision Support System for Examination of Eligibility Criteria from Securities Prospectuses at the German Central Bank\n",
       "  - Natural Language Processing for Financial Regulation\n",
       "  - DocFinQA: A Long-Context Financial Reasoning Dataset\n",
       "  - LongFin: A Multimodal Document Understanding Model for Long Financial Domain Documents\n",
       "  - Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A Comprehensive Framework and Dataset\n",
       "  - Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A Comprehensive Framework and Dataset\n",
       "  - Towards Optimizing the Costs of LLM Usage"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "  - FINRA Publishes 2024 Annual Regulatory Oversight Report | JD Supra\n",
       "  - FINRA Launches Machine-Readable Rulebook Initiative\n",
       "  - SEC Approves FINRA Rule Requiring Registration of Algorithmic Trading Developers\n",
       "  - Press Release Distribution and Management\n",
       "  - News Releases & Statements | FINRA.org\n",
       "  - Finra Calls AI ‘Emerging Risk’ in Annual Regulatory Report\n",
       "  - Fidelity launches compliant marketing business powered by AI technology\n",
       "  - SEC Approves Finra Proposals on Remote Work\n",
       "  - FINRA Fines Webull $3 Million for Options Customer Approval Violations\n",
       "  - December 2018 - Financial Regulation News"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "  - FinEAS: Financial Embedding Analysis of Sentiment\n",
       "  - BioFinBERT: Finetuning Large Language Models (LLMs) to Analyze Sentiment of Press Releases and Financial Text Around Inflection Points of Biotech Stocks\n",
       "  - Is Domain Adaptation Worth Your Investment? Comparing BERT and FinBERT on Financial Tasks\n",
       "  - Term Expansion and FinBERT fine-tuning for Hypernym and Synonym Ranking of Financial Terms\n",
       "  - Yseop at FinSim-3 Shared Task 2021: Specializing Financial Domain Learning with Phrase Representations\n",
       "  - GitHub - keitazoumana/finBERT-Implementation: This repository is the illustration of finBERT for financial document classification\n",
       "  - FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets\n",
       "  - Sentiment-driven prediction of financial returns: a Bayesian-enhanced FinBERT approach\n",
       "  - A Comparison of Pre-Trained Language Models for Multi-Class Text Classification in the Financial Domain\n",
       "  - German FinBERT: A German Pre-trained Language Model\n",
       "  - Text Mining of Stocktwits Data for Predicting Stock Prices\n",
       "  - GOAT at the FinSim-2 task: Learning Word Representations of Financial Data with Customized Corpus"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[Step 4] Draft:** \n",
       "\n",
       "This comprehensive analysis examines different approaches for using Large Language Models (LLMs) in financial document summarization, comparing traditional techniques, finance-specific models, and hybrid systems.\n",
       "\n",
       "Key Findings:\n",
       "\n",
       "1. Performance of Finance-Specific Models:\n",
       "- FinBERT and other finance-tuned models show improved but not dramatically better performance compared to general LLMs\n",
       "- Domain adaptation through continued pre-training from base models outperforms training from scratch on financial data\n",
       "- Recent evaluations show GPT-4-Turbo with retrieval still has significant limitations, incorrectly handling 81% of financial QA tasks\n",
       "\n",
       "2. Hybrid Approaches:\n",
       "- Combining LLMs with rule-based systems shows promise for handling domain-specific requirements\n",
       "- The AFIE framework combining LLMs with traditional extractive techniques achieved 33-53% accuracy improvements\n",
       "- Systems like Saifr demonstrate successful integration of NLP and compliance rules for financial content\n",
       "\n",
       "3. Document Length Challenges:\n",
       "- Most models struggle with long financial documents (>512 tokens)\n",
       "- New datasets like DocFinQA and LongFin are helping evaluate and improve long-context understanding\n",
       "- Specialized architectures like LongFin can handle up to 4K tokens while maintaining accuracy\n",
       "\n",
       "4. Practical Considerations:\n",
       "- Cost optimization is critical - studies show potential 40-90% cost reduction while maintaining quality\n",
       "- Regulatory compliance requires careful integration of rules and human oversight\n",
       "- Class imbalance and domain-specific terminology remain significant challenges\n",
       "\n",
       "5. Implementation Best Practices:\n",
       "- Use domain-specific fine-tuning rather than training from scratch\n",
       "- Implement hybrid architectures combining LLMs with rule-based systems\n",
       "- Consider document length limitations and cost implications\n",
       "- Maintain human oversight for regulatory compliance\n",
       "- Regular evaluation against financial domain benchmarks\n",
       "\n",
       "Conclusion:\n",
       "While pure LLM approaches show promise, hybrid systems combining LLMs with rule-based components currently offer the most practical path forward for financial document summarization. Key challenges around document length, cost optimization, and regulatory compliance require careful architectural consideration. Regular evaluation against domain-specific benchmarks remains essential as the technology evolves."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Source:** [FinanceBench: A New Benchmark for Financial Question Answering](https://arxiv.org/abs/2311.11944)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Source:** [DocFinQA: A Long-Context Financial Reasoning Dataset](https://arxiv.org/abs/2401.06915)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Source:** [Towards Optimizing the Costs of LLM Usage](https://arxiv.org/abs/2402.01742)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Source:** [LongFin: A Multimodal Document Understanding Model for Long Financial Domain Documents](http://arxiv.org/abs/2401.15050)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Source:** [Is Domain Adaptation Worth Your Investment? Comparing BERT and FinBERT on Financial Tasks](https://www.aclanthology.org/2021.econlp-1.5.pdf)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now call your research function\n",
    "result = await research(\n",
    "    branch=researcher,\n",
    "    query=query_text,\n",
    "    domain=domain_hint,\n",
    "    style=style_hint,\n",
    "    sample_writing=sample_snippet,\n",
    "    interpret_kwargs={\"temperature\": 0.3},  # example\n",
    "    verbose=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
